# API Configuration
API_PORT=8000
UI_PORT=3000

# LLM Configuration
LLM_BACKEND=ollama  # Options: ollama, cloud (stub)
OLLAMA_MODEL=phi3:mini  # Small model for quick responses
TEMPERATURE=0.6
MAX_TOKENS=150

# Optional
SAVE_TRANSCRIPTS=false